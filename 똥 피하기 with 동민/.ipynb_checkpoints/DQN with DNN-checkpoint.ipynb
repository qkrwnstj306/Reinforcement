{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "33a12b84",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import collections # collections 모듈에 있는 deque를 사용하기 위함\n",
    "#dequq는 double-ended queue로써, queue와 stack의 기능을 다 쓸 수 있다.\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import random\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "88a6a34b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class enva():\n",
    "    def __init__(self):\n",
    "        self.height = 10\n",
    "        self.width = 10\n",
    "        self.num_obstacle_range =5\n",
    "        self.num_obstacle_range_min = 3\n",
    "        self.turn = 0\n",
    "        self.position = int(round(self.width/2))\n",
    "        self.map = np.array([[0.0 for j in range(self.width)] for i in range(self.height)])\n",
    "        self.map[self.height-1][self.position] = 2.\n",
    "        self.done = False\n",
    "        \n",
    "    def step(self, key):\n",
    "        #obstacle down\n",
    "        i = self.height - 2\n",
    "        while(i>=0):\n",
    "            self.map[i+1] = self.map[i]\n",
    "            i -= 1\n",
    "        #obstacle init(first_line)\n",
    "        self.map[0] = np.array([0 for j in range(self.width)])\n",
    "        #obstacle making\n",
    "        num_obstacle = random.randrange(self.num_obstacle_range_min,self.num_obstacle_range)\n",
    "        i = 0\n",
    "        while(i < num_obstacle):\n",
    "            position_obstacle = random.randrange(0,self.width)\n",
    "            if (self.map[0][position_obstacle] == 1.0):\n",
    "                continue\n",
    "            self.map[0][position_obstacle] = 1.0\n",
    "            i += 1\n",
    "        #big_obstacle\n",
    "        big_obstacle = random.randrange(0,20)\n",
    "        if(big_obstacle == -1): ###############\n",
    "            big_obstacle = random.randrange(1,self.width-1)\n",
    "            for j in range(3):\n",
    "                for k in range(-1,2):\n",
    "                    self.map[j][big_obstacle + k] = 1.\n",
    "        #player position\n",
    "        reward = 1\n",
    "        # 0 : 왼쪽, 1 : 가만히, 2 : 오른쪽\n",
    "        if key == 0:\n",
    "            if(self.position>0):\n",
    "                self.position -= 1\n",
    "            \n",
    "        if key == 1:\n",
    "            pass      \n",
    "        if key == 2:\n",
    "            if(self.position<self.width-1):\n",
    "                self.position += 1\n",
    "                  \n",
    "        if (self.map[self.height-1][self.position] == 1.0):\n",
    "                    reward = 0\n",
    "                    self.done = True\n",
    "                    #print(\"====Game Over====\")\n",
    "                        \n",
    "        self.map[self.height-1][self.position] = 2.0\n",
    "        #reward(turn(time))\n",
    "        self.turn += 1\n",
    "        \n",
    "        return torch.flatten(torch.tensor(self.map),0), reward, self.done, _ \n",
    "\n",
    "    def reset(self):\n",
    "        self.turn = 0\n",
    "        self.position = round(self.width/2)\n",
    "        self.map = np.array([[0.0 for j in range(self.width)] for i in range(self.height)])\n",
    "        self.map[self.height-1][self.position] = 2.0\n",
    "        self.done = False\n",
    "        \n",
    "        return torch.flatten(torch.tensor(self.map),0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ef858713",
   "metadata": {},
   "outputs": [],
   "source": [
    "#nerual network setting -> good performance\n",
    "#hyperparameters\n",
    "buffer_size =30000   # environment's size에 맞춰서 setting\n",
    "batch_size = 32   \n",
    "gamma = 0.98  # Large future weight\n",
    "\n",
    "class Agent(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Agent,self).__init__()\n",
    "        self.fc1 = nn.Linear(100,128)\n",
    "        self.fc2 = nn.Linear(128,128)\n",
    "        self.fc3 = nn.Linear(128,128)\n",
    "        self.fc4 = nn.Linear(128,3)  # 좌우\n",
    "  \n",
    "    def forward(self,x):\n",
    "        \n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = self.fc4(x)\n",
    "        return x\n",
    "  \n",
    "  \n",
    "    def select_action(self,state,eps):   # state, reward를 받으면 action이 나오는 relation. \n",
    "                                        # but reward는 memory를 통해 사용하므로 state를 받아서 action을 뽑는다.\n",
    "\n",
    "        coin = random.random() # 0~1\n",
    "        out_value = self.forward(state) # \n",
    "        if coin < eps:   # random action\n",
    "            a = random.randint(0,2) # 0 ,1 ,2 choice\n",
    "        else : # optimize action\n",
    "            a = torch.argmax(out_value).item() # a는 tensor가 아닌 int로 들어가야 한다.\n",
    "                                               # 둘 중 높은 value를 가진 index가 action이다. \n",
    "        return a\n",
    "\n",
    "class replay_buffer():\n",
    "    def __init__(self):\n",
    "        self.buffer = collections.deque(maxlen = buffer_size)\n",
    "    def put(self,input): # buffer에 store\n",
    "        self.buffer.append(input) # dtype = tensor  4개가 한 묶음으로 들어가야됨\n",
    "\n",
    "    def sampling(self,bat_ch): \n",
    "        mini_batch = random.sample(self.buffer,bat_ch) # bat_ch X 4 (list), dtype = tensor\n",
    "                                                       # buffer에서 batch_size (현재 32)만큼 sampling을 한다.\n",
    "        #s_list, a_list, r_prime_list, s_prime_list,done_mask_lst = [],[],[],[],[] # list로 각각을 저장할거임\n",
    "        a_list, r_prime_list, done_mask_lst = [],[],[]\n",
    "        s_list = torch.tensor([0 for i in range(100)],dtype = torch.float).unsqueeze(dim=0)\n",
    "        s_prime_list = torch.tensor([0 for i in range(100)],dtype = torch.float).unsqueeze(dim=0)\n",
    "        \n",
    "        for transition in mini_batch:\n",
    "              s, a, r_prime, s_prime,done_mask = transition\n",
    "              \n",
    "              \n",
    "              # a : int, r_prime : float\n",
    "              s_list = torch.cat([s_list,s.unsqueeze(dim=0)],dim=0) \n",
    "              s_prime_list = torch.cat([s_prime_list,s_prime.unsqueeze(dim=0)],dim=0)         \n",
    "              #s_list.append(s)\n",
    "              a_list.append([a]) # 열로 쌓는다\n",
    "              r_prime_list.append([r_prime])\n",
    "              #s_prime_list.append(s_prime)\n",
    "              done_mask_lst.append([done_mask])\n",
    "        s_list = s_list[1:,:]\n",
    "        s_prime_list = s_prime_list[1:,:]      \n",
    "        return torch.tensor(s_list, dtype=torch.float), torch.tensor(a_list), \\\n",
    "                       torch.tensor(r_prime_list), torch.tensor(s_prime_list, dtype=torch.float),torch.tensor(done_mask_lst)\n",
    "\n",
    "    def size(self):\n",
    "        return len(self.buffer)\n",
    "\n",
    "def train(q,q_target,disk,optimizer):\n",
    "    for i in range(10):\n",
    "        s, a, r, s_prime,done_mask = disk.sampling(batch_size) # s : batch_size x 4    \n",
    "\n",
    "        q_out = q(s) # batch_size x 4 \n",
    "        q_a = q_out.gather(1,a) # dim =1에서 index가 a인 것들로 재구성\n",
    "        #print(q_a.shape)  # batch_size x 1\n",
    "        qtarget_out = q_target(s_prime) # s'에서의 value가 4개 나옴 -> batch_size x 4\n",
    "        #print(qtarget_out.max(1)) 값 반환 하는데, 이 값은 행벡터\n",
    "        max_q_prime = qtarget_out.max(1)[0].unsqueeze(1) # dim = 1 에서 max값 가지고 온다. batch_size x 1로 만듦\n",
    "\n",
    "        target = r + max_q_prime*gamma* done_mask \n",
    "        loss = F.smooth_l1_loss(q_a,target)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "43f7017e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_episode :100, score : 10.9, n_buffer : 1194,eps = 0.075\n",
      "n_episode :200, score : 11.1, n_buffer : 2399,eps = 0.07\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1638931/3510447009.py:64: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return torch.tensor(s_list, dtype=torch.float), torch.tensor(a_list), \\\n",
      "/tmp/ipykernel_1638931/3510447009.py:65: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  torch.tensor(r_prime_list), torch.tensor(s_prime_list, dtype=torch.float),torch.tensor(done_mask_lst)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_episode :300, score : 11.3, n_buffer : 3634,eps = 0.065\n",
      "n_episode :400, score : 11.5, n_buffer : 4880,eps = 0.06\n",
      "n_episode :500, score : 11.4, n_buffer : 6118,eps = 0.055\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 19\u001b[0m\n\u001b[1;32m     16\u001b[0m one_score \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m done :\n\u001b[0;32m---> 19\u001b[0m   a \u001b[38;5;241m=\u001b[39m \u001b[43mq\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mselect_action\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43meps\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     20\u001b[0m   s_prime,r,done,info \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mstep(a)\n\u001b[1;32m     21\u001b[0m   done_mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m done \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m1.0\u001b[39m  \u001b[38;5;66;03m# termination이면 done을 0으로..\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[5], line 28\u001b[0m, in \u001b[0;36mAgent.select_action\u001b[0;34m(self, state, eps)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mselect_action\u001b[39m(\u001b[38;5;28mself\u001b[39m,state,eps):   \u001b[38;5;66;03m# state, reward를 받으면 action이 나오는 relation. \u001b[39;00m\n\u001b[1;32m     25\u001b[0m                                     \u001b[38;5;66;03m# but reward는 memory를 통해 사용하므로 state를 받아서 action을 뽑는다.\u001b[39;00m\n\u001b[1;32m     27\u001b[0m     coin \u001b[38;5;241m=\u001b[39m random\u001b[38;5;241m.\u001b[39mrandom() \u001b[38;5;66;03m# 0~1\u001b[39;00m\n\u001b[0;32m---> 28\u001b[0m     out_value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# \u001b[39;00m\n\u001b[1;32m     29\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m coin \u001b[38;5;241m<\u001b[39m eps:   \u001b[38;5;66;03m# random action\u001b[39;00m\n\u001b[1;32m     30\u001b[0m         a \u001b[38;5;241m=\u001b[39m random\u001b[38;5;241m.\u001b[39mrandint(\u001b[38;5;241m0\u001b[39m,\u001b[38;5;241m2\u001b[39m) \u001b[38;5;66;03m# 0 ,1 ,2 choice\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[5], line 18\u001b[0m, in \u001b[0;36mAgent.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m,x):\n\u001b[1;32m     17\u001b[0m     x \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc1(x))\n\u001b[0;32m---> 18\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrelu\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfc2\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     19\u001b[0m     x \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc3(x))\n\u001b[1;32m     20\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc4(x)\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.8/site-packages/torch/nn/functional.py:1136\u001b[0m, in \u001b[0;36mrelu\u001b[0;34m(input, inplace)\u001b[0m\n\u001b[1;32m   1134\u001b[0m     result \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrelu_(\u001b[38;5;28minput\u001b[39m)\n\u001b[1;32m   1135\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1136\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrelu\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1137\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "env = enva()\n",
    "q = Agent()\n",
    "q_target = Agent()\n",
    "q_target.load_state_dict(q.state_dict())\n",
    "memory = replay_buffer()  \n",
    "\n",
    "optimizer = optim.Adam(q.parameters(),lr = 0.0005)\n",
    "score = 0.0\n",
    "\n",
    "for epi in range(30000):\n",
    "\n",
    "  eps = max(0.0, 0.08 - 0.01*(epi/200)) #0,08\n",
    "  s = env.reset()\n",
    "   \n",
    "  done = False\n",
    "  one_score = 0\n",
    "  while not done :\n",
    "   \n",
    "    a = q.select_action(s.float(),eps)\n",
    "    s_prime,r,done,info = env.step(a)\n",
    "    done_mask = 0.0 if done else 1.0  # termination이면 done을 0으로..\n",
    "    memory.put((s,a,r,s_prime, done_mask)) # state, action, reward, state', done_mask\n",
    "    s = s_prime\n",
    "    score += r\n",
    "    one_score += r\n",
    "  if one_score > 5000:\n",
    "            break\n",
    "  if memory.size() >3000 :\n",
    "    train(q,q_target,memory,optimizer)\n",
    "\n",
    "  if epi%100==0 and epi!=0:\n",
    "        q_target.load_state_dict(q.state_dict())\n",
    "        print(\"n_episode :{}, score : {:.1f}, n_buffer : {},eps = {}\".format(\n",
    "                                                        epi, score/100, memory.size(),eps))\n",
    "        \n",
    "        score = 0.0\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4865eec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from time import sleep\n",
    "import numpy as np\n",
    "import random\n",
    "import keyboard\n",
    "import copy\n",
    "\n",
    "height = 10\n",
    "width = 10\n",
    "num_obstacle_range =7\n",
    "num_obstacle_range_min = 4\n",
    "turn = 0\n",
    "position = round(width/2)\n",
    "player_position = round(width/2)\n",
    "map = np.array([[0. for j in range(width)] for i in range(height)])\n",
    "map[height-1][position] = 2.\n",
    "\n",
    "player_map = copy.deepcopy(map)\n",
    "q.eval()\n",
    "with torch.no_grad():\n",
    "\n",
    "    while True:\n",
    "        #print_map\n",
    "        action = q.forward(torch.flatten(torch.tensor(map).float()))\n",
    "        print(\"===============ai map==================\")\n",
    "        print(map)\n",
    "        print(\"===============player map==================\")\n",
    "        print(player_map)\n",
    "        print(\"Turn:\" ,turn)\n",
    "        #obstacle down\n",
    "        i = height - 2\n",
    "        while(i>=0):\n",
    "            map[i+1] = map[i]\n",
    "            i -= 1\n",
    "        #obstacle init(first_line)\n",
    "        map[0] = np.array([0. for j in range(width)])\n",
    "        #obstacle making\n",
    "        num_obstacle = random.randrange(num_obstacle_range_min,num_obstacle_range)\n",
    "        i = 0\n",
    "        while(i < num_obstacle):\n",
    "            position_obstacle = random.randrange(0,width)\n",
    "            if (map[0][position_obstacle] == 1.):\n",
    "                continue\n",
    "            map[0][position_obstacle] = 1.\n",
    "            i += 1\n",
    "        #big_obstacle\n",
    "        big_obstacle = random.randrange(0,20)\n",
    "        if(big_obstacle == 0):\n",
    "            big_obstacle = random.randrange(1,width-1)\n",
    "            for j in range(3):\n",
    "                for k in range(-1,2):\n",
    "                    map[j][big_obstacle + k] = 1.\n",
    "        #copy player_map\n",
    "        player_map = copy.deepcopy(map)\n",
    "        #ai position\n",
    "        #print(\"value : \",action)\n",
    "        action = torch.argmax(action).item()\n",
    "        #print(\"action : \", action)\n",
    "        \n",
    "        while True:\n",
    "            player_key = input()\n",
    "            key = action\n",
    "            # player action\n",
    "            if player_key == \"j\":\n",
    "                if(player_position>0):\n",
    "                    player_position -= 1\n",
    "                \n",
    "            if player_key == \"k\":\n",
    "                pass        \n",
    "            if player_key == \"l\":\n",
    "                if(player_position<width-1):\n",
    "                    player_position += 1\n",
    "                        \n",
    "\n",
    "            # ai action\n",
    "            if key == 0:\n",
    "                if(position>0):\n",
    "                    position -= 1\n",
    "                break\n",
    "            if key == 1:\n",
    "                break        \n",
    "            if key ==2:\n",
    "                if(position<width-1):\n",
    "                    position += 1\n",
    "                break        \n",
    "        if map[height-1][position] == 1 or player_map[height-1][player_position] == 1:\n",
    "                    print(\"====Game Over====\")\n",
    "                    if map[height-1][position] == 1:\n",
    "                        print(\"player win!!\\n\\n\")\n",
    "                    \n",
    "                    elif player_map[height-1][player_position] == 1:\n",
    "                        print(\"AI win!!\\n\\n\")\n",
    "                    sys.exit()    \n",
    "        map[height-1][position] = 2.\n",
    "        player_map[height-1][player_position] = 2.\n",
    "        #reward(turn(time))\n",
    "        turn += 1\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d139a3d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bd13678",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
