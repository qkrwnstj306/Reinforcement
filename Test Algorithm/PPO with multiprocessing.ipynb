{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98b100d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import copy\n",
    "from torch.distributions import Categorical\n",
    "import torch.multiprocessing as mp\n",
    "import time\n",
    "\n",
    "# Hyperparameters\n",
    "n_train_processes = 3\n",
    "learning_rate = 0.0001\n",
    "gamma = 0.98\n",
    "max_train_ep = 2000\n",
    "max_test_ep = 3000\n",
    "K_epoch = 3\n",
    "T_horizon = 20\n",
    "eps_clip = 0.1\n",
    "lmbda = 0.95\n",
    "\n",
    "class PPO(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(PPO, self).__init__()\n",
    "        self.data = []\n",
    "\n",
    "        self.fc1 = nn.Linear(4, 256)\n",
    "        self.fc_pi = nn.Linear(256, 2)\n",
    "        self.fc_v = nn.Linear(256, 1)\n",
    "\n",
    "    def pi(self, x, softmax_dim=0):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc_pi(x)\n",
    "        prob = F.softmax(x, dim=softmax_dim)\n",
    "        return prob\n",
    "\n",
    "    def v(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        v = self.fc_v(x)\n",
    "        return v\n",
    "\n",
    "    def put_data(self, transition):\n",
    "        self.data.append(transition)\n",
    "        \n",
    "    def make_batch(self):\n",
    "        s_lst, a_lst, r_lst, s_prime_lst, prob_a_lst, done_lst = [], [], [], [], [], []\n",
    "        for transition in self.data:\n",
    "            s, a, r, s_prime, prob_a, done = transition\n",
    "            \n",
    "            s_lst.append(s)\n",
    "            a_lst.append([a])\n",
    "            r_lst.append([r])\n",
    "            s_prime_lst.append(s_prime)\n",
    "            prob_a_lst.append([prob_a])\n",
    "            done_mask = 0 if done else 1\n",
    "            done_lst.append([done_mask])\n",
    "            \n",
    "        s,a,r,s_prime,done_mask, prob_a = torch.tensor(s_lst, dtype=torch.float), torch.tensor(a_lst), \\\n",
    "                                          torch.tensor(r_lst), torch.tensor(s_prime_lst, dtype=torch.float), \\\n",
    "                                          torch.tensor(done_lst, dtype=torch.float), torch.tensor(prob_a_lst)\n",
    "        self.data = []\n",
    "        return s, a, r, s_prime, done_mask, prob_a\n",
    "    \n",
    "\n",
    "def train(global_model, rank):\n",
    "    local_model = PPO()\n",
    "    local_model.load_state_dict(global_model.state_dict())\n",
    "\n",
    "    optimizer = optim.Adam(global_model.parameters(), lr=learning_rate)\n",
    "    \n",
    "    env = gym.make('CartPole-v1')\n",
    "\n",
    "    for n_epi in range(max_train_ep):\n",
    "        done = False\n",
    "        s = env.reset()\n",
    "        \n",
    "        while not done:\n",
    "            for t in range(T_horizon):\n",
    "                \n",
    "                prob = local_model.pi(torch.from_numpy(s).float())\n",
    "                m = Categorical(prob)\n",
    "                a = m.sample().item()\n",
    "                s_prime, r, done, info = env.step(a)\n",
    "\n",
    "                local_model.put_data((s,a,r,s_prime,prob[a].item(),done))\n",
    "                s = s_prime.copy()\n",
    "                \n",
    "                if done:\n",
    "                    break\n",
    "\n",
    "            s_batch, a_batch, r_batch, s_prime_batch, done_mask_batch, prob_a_batch = local_model.make_batch()\n",
    "            \n",
    "            #for i in range(K_epoch):\n",
    "            td_target = r_batch + gamma * local_model.v(s_prime_batch) * done_mask_batch\n",
    "            delta = td_target - local_model.v(s_batch)\n",
    "            delta = delta.detach().numpy()\n",
    "\n",
    "            advantage_lst = []\n",
    "            advantage = 0.0\n",
    "            for delta_t in delta[::-1]:\n",
    "                advantage = gamma * lmbda * advantage + delta_t[0]\n",
    "                advantage_lst.append([advantage])\n",
    "            advantage_lst.reverse()\n",
    "            advantage = torch.tensor(advantage_lst, dtype=torch.float)\n",
    "\n",
    "            pi = local_model.pi(s_batch, softmax_dim=1)\n",
    "            pi_a = pi.gather(1,a_batch)\n",
    "            ratio = torch.exp(torch.log(pi_a) - torch.log(prob_a_batch))  # a/b == exp(log(a)-log(b))\n",
    "           \n",
    "            surr1 = ratio * advantage\n",
    "            surr2 = torch.clamp(ratio, 1-eps_clip, 1+eps_clip) * advantage\n",
    "            loss = -torch.min(surr1, surr2) + F.smooth_l1_loss(local_model.v(s_batch) , td_target.detach())\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.mean().backward()\n",
    "            #optimizer.step()\n",
    "            for global_param, local_param in zip(global_model.parameters(), local_model.parameters()):\n",
    "                global_param._grad = local_param.grad\n",
    "            optimizer.step()\n",
    "            local_model.load_state_dict(global_model.state_dict())\n",
    "           \n",
    "\n",
    "    env.close()\n",
    "    print(\"Training process {} reached maximum episode.\".format(rank))\n",
    "\n",
    "\n",
    "def test(global_model):\n",
    "    env = gym.make('CartPole-v1')\n",
    "    score = 0.0\n",
    "    print_interval = 20\n",
    "\n",
    "    for n_epi in range(max_test_ep):\n",
    "        done = False\n",
    "        s = env.reset()\n",
    "\n",
    "        while not done:\n",
    "            \n",
    "            prob = global_model.pi(torch.from_numpy(s).float())\n",
    "            a = Categorical(prob).sample().item()\n",
    "            s_prime, r, done, info = env.step(a)\n",
    "            s = s_prime\n",
    "            score += r\n",
    "\n",
    "        if n_epi % print_interval == 0 and n_epi != 0:\n",
    "            print(\"# of episode :{}, avg score : {:.1f}\".format(\n",
    "                n_epi, score/print_interval))\n",
    "            score = 0.0\n",
    "            time.sleep(1)\n",
    "    env.close()\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    global_model = PPO()\n",
    "    global_model.share_memory()\n",
    "\n",
    "    processes = []\n",
    "    for rank in range(n_train_processes + 1):  # + 1 for test process\n",
    "        if rank == 0:\n",
    "            p = mp.Process(target=test, args=(global_model,))\n",
    "        else:\n",
    "            p = mp.Process(target=train, args=(global_model, rank,))\n",
    "        p.start()\n",
    "        processes.append(p)\n",
    "    for p in processes:\n",
    "        p.join()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
