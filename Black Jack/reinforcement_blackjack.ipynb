{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f7a25dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "class gridworld():\n",
    "  def __init__(self):\n",
    "    self.x=0\n",
    "    self.y=0\n",
    "  def step(self,a):\n",
    "    if a==0:\n",
    "      self.move_right()\n",
    "    elif a==1:\n",
    "      self.move_left()\n",
    "    elif a==2:\n",
    "      self.move_up()\n",
    "    elif a==3:\n",
    "      self.move_down()\n",
    "    \n",
    "    reward = -1\n",
    "    done = self.is_done()\n",
    "    return (self.x, self.y), reward, done\n",
    "  \n",
    "  def move_right(self):\n",
    "    self.y +=1\n",
    "    if self.y>3:\n",
    "      self.y = 3\n",
    "  def move_left(self):\n",
    "    self.y -=1\n",
    "    if self.y<0:\n",
    "      self.y = 0\n",
    "  def move_up(self):\n",
    "    self.x -=1\n",
    "    if self.x<0:\n",
    "      self.x = 0\n",
    "  def move_down(self):\n",
    "    self.x +=1\n",
    "    if self.x>3:\n",
    "      self.x = 3\n",
    "  def is_done(self):\n",
    "    if self.x==3 and self.y==3:\n",
    "      return True\n",
    "    else :\n",
    "      return False\n",
    "  \n",
    "  def get_state(self):\n",
    "    return (self.x,self.y)\n",
    "  \n",
    "  def reset(self):\n",
    "    self.x =0\n",
    "    self.y =0\n",
    "    return (self.x,self.y)\n",
    "\n",
    "\n",
    "class Agent():\n",
    "  def __init__(self):\n",
    "    pass\n",
    "  def select_action(self):\n",
    "    coin = random.random()\n",
    "    if coin <0.25:\n",
    "      action =0\n",
    "    elif coin <0.5 :\n",
    "      action =1\n",
    "    elif coin <0.75 :\n",
    "      action =2\n",
    "    else :\n",
    "      action =3\n",
    "    return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6b51d5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(): # Monte-Carlo prediction\n",
    "  env = gridworld()\n",
    "  agent = Agent()\n",
    "  data = np.zeros((4,4))\n",
    "  gamma = 1.0\n",
    "  alpha =0.0001\n",
    "\n",
    "  for k in range(50000):\n",
    "    done = False\n",
    "    history=[]\n",
    "    while not done:\n",
    "      action = agent.select_action()\n",
    "      (x, y), reward, done = env.step(action)\n",
    "      history.append((x,y,reward))\n",
    "    env.reset() #episode가 끝났으므로, agent의 위치 초기화\n",
    "\n",
    "    cum_reward = 0 # Gt | termination부터 계산\n",
    "    for transition in history[::-1]:\n",
    "      x,y,reward = transition\n",
    "      data[x][y] = data[x][y] + alpha*(cum_reward - data[x][y])\n",
    "      cum_reward = reward + gamma*cum_reward # action에 대한 reward + G(t+1) = Gt\n",
    "\n",
    "  for row in data:\n",
    "    print(row)\n",
    "\n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00b5ad36",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():#TD prediction\n",
    "  env = gridworld()\n",
    "  agent = Agent()\n",
    "  data = np.zeros((4,4))\n",
    "  gamma= 1.0\n",
    "  alpha = 0.01 #MC에 비해 큰 값 사용\n",
    "\n",
    "  for k in range(50000):\n",
    "    done = False\n",
    "    while not done:\n",
    "      x,y = env.get_state()  # state에서 action을 하고 움직이기 전 state를 update하기 위해서, 값 저장\n",
    "      action = agent.select_action()\n",
    "      (x_prime,y_prime),reward,done = env.step(action)\n",
    "      x_prime,y_prime = env.get_state()\n",
    "\n",
    "      data[x][y] = data[x][y] + alpha*(reward + gamma*data[x_prime][y_prime]-data[x][y])  # action하고 state update\n",
    "    env.reset()\n",
    "  for row in data:\n",
    "    print(row)\n",
    "\n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fce470df",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random \n",
    "import numpy as np\n",
    "\n",
    "class gridworld2():\n",
    "  def __init__(self):\n",
    "    self.x=0\n",
    "    self.y=0\n",
    "  def step(self,a):\n",
    "    if a==0:\n",
    "      self.move_left()\n",
    "    elif a==1:\n",
    "      self.move_up()\n",
    "    elif a==2:\n",
    "      self.move_right()\n",
    "    elif a==3:\n",
    "      self.move_down()\n",
    "    \n",
    "    reward = -1\n",
    "    done = self.is_done()\n",
    "    return (self.x, self.y), reward, done\n",
    "  \n",
    "  def move_left(self):\n",
    "    if self.y==0:\n",
    "      pass\n",
    "    elif self.y==3 and self.x in [0,1,2]:\n",
    "      pass\n",
    "    elif self.y==5 and self.x in [2,3,4]:\n",
    "      pass\n",
    "    else:\n",
    "      self.y -=1\n",
    "  def move_right(self):\n",
    "    if self.y==1 and self.x in [0,1,2]:\n",
    "      pass\n",
    "    elif self.y==3 and self.x in [2,3,4]:\n",
    "      pass\n",
    "    elif self.y==6:\n",
    "      pass\n",
    "    else:\n",
    "      self.y +=1\n",
    "  def move_up(self):\n",
    "    if self.x==0:\n",
    "      pass\n",
    "    elif self.x==3 and self.y==2:\n",
    "      pass\n",
    "    else:\n",
    "      self.x -=1\n",
    "  def move_down(self):\n",
    "    if self.x==4:\n",
    "      pass\n",
    "    elif self.x==1 and self.y==4:\n",
    "      pass\n",
    "    else:\n",
    "      self.x+=1\n",
    "  def is_done(self):\n",
    "    if self.x==4 and self.y==6:\n",
    "      return True\n",
    "    else:\n",
    "      return False\n",
    "  def reset(self):\n",
    "    self.x=0\n",
    "    self.y=0\n",
    "    return (self.x,self.y)\n",
    "\n",
    "class QAgent() :\n",
    "  def __init__(self):\n",
    "    self.q_table = np.zeros((5,7,4))\n",
    "    self.eps = 0.9\n",
    "    self.alpha = 0.01\n",
    "\n",
    "  def select_action(self,s):\n",
    "    x,y =s\n",
    "    coin = random.random()\n",
    "    if coin<self.eps :\n",
    "      action = random.randint(0,3)\n",
    "    else :\n",
    "      action_val = self.q_table[x,y,:]\n",
    "      action = np.argmax(action_val)\n",
    "    return action\n",
    "  def update_table(self,history):\n",
    "    cum_reward = 0\n",
    "    for transition in history[::-1]:\n",
    "      s,a,r,s_prime = transition\n",
    "      x,y =s \n",
    "      self.q_table[x,y,a] = self.q_table[x,y,a]+self.alpha*(cum_reward-self.q_table[x,y,a])\n",
    "      cum_reward = cum_reward+r\n",
    "  def anneal_eps(self):\n",
    "    self.eps -=0.03\n",
    "    self.eps = max(self.eps,0.1)\n",
    "  def show_table(self):\n",
    "    q_lst = self.q_table.tolist()\n",
    "    data = np.zeros((5,7))\n",
    "    for row_idx in range(len(q_lst)):\n",
    "      row=q_lst[row_idx]\n",
    "      for col_idx in range(len(row)):\n",
    "        col = row[col_idx]\n",
    "        action = np.argmax(col)\n",
    "        data[row_idx,col_idx] = action\n",
    "    print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7061fed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(): #MC로 control\n",
    "  env = gridworld2()\n",
    "  agent = QAgent()\n",
    "\n",
    "  for n_epi in range(1000):\n",
    "    done = False\n",
    "    history = []\n",
    "    s = env.reset()\n",
    "    while not done:\n",
    "      a = agent.select_action(s)\n",
    "      s_prime,r,done = env.step(a)\n",
    "      history.append((s,a,r,s_prime))\n",
    "      s = s_prime\n",
    "    agent.update_table(history)\n",
    "    agent.anneal_eps()\n",
    "\n",
    "  agent.show_table()\n",
    "\n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a55a5003",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QAgent2():\n",
    "  def __init__(self):\n",
    "    self.q_table = np.zeros((5,7,4))\n",
    "    self.eps = 0.9\n",
    "  def select_action(self,s):\n",
    "    x,y=s\n",
    "    coin = random.random()\n",
    "    if coin<self.eps:\n",
    "      action = random.randint(0,3)\n",
    "    else :\n",
    "      action_val = self.q_table[x,y,:]\n",
    "      action = np.argmax(action_val)\n",
    "    return action\n",
    "  def update_table(self,transition):\n",
    "    s,a,r,s_prime = transition\n",
    "    x,y=s\n",
    "    next_x,next_y = s_prime\n",
    "    a_prime = self.select_action(s_prime)\n",
    "\n",
    "    self.q_table[x,y,a] = self.q_table[x,y,a]+0.1*(r+self.q_table[next_x,next_y,a_prime]-self.q_table[x,y,a])\n",
    "\n",
    "  def anneal_eps(self):\n",
    "    self.eps -=0.03\n",
    "    self.eps = max(self.eps,0.1)\n",
    "  \n",
    "  def show_table(self):\n",
    "    q_lst = self.q_table.tolist()\n",
    "    data = np.zeros((5,7))\n",
    "    for row_idx in range(len(q_lst)):\n",
    "      row = q_lst[row_idx]\n",
    "      for col_idx in range(len(row)):\n",
    "        col = row[col_idx]\n",
    "        action = np.argmax(col)\n",
    "        data[row_idx,col_idx] = action\n",
    "    print(data)\n",
    "\n",
    "def main(): #TD control\n",
    "  env = gridworld2()\n",
    "  agent = QAgent2()\n",
    "\n",
    "  for n_epi in range(1000):\n",
    "    done = False\n",
    "    s = env.reset()\n",
    "    while not done:\n",
    "      a = agent.select_action(s)\n",
    "      s_prime,r,done = env.step(a)\n",
    "      agent.update_table((s,a,r,s_prime))\n",
    "      s = s_prime\n",
    "    agent.anneal_eps()\n",
    "  agent.show_table()\n",
    "\n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "789aba47",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random # ====================================dealer가 game시작 전에 이미 카드를 모두 받은 상황\n",
    "import numpy as np\n",
    "\n",
    "class blackJack():\n",
    "  \n",
    "  def __init__(self): #trash value\n",
    "    self.x=[]  # user sum\n",
    "    self.y=0  # dealer show\n",
    "    self.y_sum =[]\n",
    "    self.z=0  # usable ace -> 11\n",
    "\n",
    "  def use_ace(self): # ace 유무 판단 -> 1이 있고,\n",
    "    if 1 in self.x and (sum(self.x) + 10 <= 21): \n",
    "      return True #ace를 11로 써도 된다.\n",
    "    elif 1 in self.x and (sum(self.x)+10 >21):\n",
    "      return False #ace를 가지고 있지만 1로 써야 한다.\n",
    "    else :\n",
    "      return False\n",
    "  \n",
    "  def dealer_ace(self) : # sum이 21을 넘지 않는 한 무조건 11로 계산\n",
    "    if 1 in self.y_sum and (sum(self.x) + 10 <= 21): \n",
    "      return True\n",
    "    elif 1 in self.y_sum and (sum(self.y_sum)+10 >21):\n",
    "      return False #ace를 가지고 있지만 1로 써야 한다.\n",
    "    else :\n",
    "      return False\n",
    "  def score_y(self): # ace 유무를 판단해서, 점수 합산 -> z도 같이 바꿔준다. 따라서 카드를 뽑아서 score로 판단\n",
    "    Bool2 = self.dealer_ace()\n",
    "    if Bool2 == True: # ace를 11로 쓰겠다\n",
    "      return int(sum(self.y_sum)+10)\n",
    "    else :\n",
    "      return int((sum(self.y_sum)))\n",
    "  def score(self): # ace 유무를 판단해서, 점수 합산 -> z도 같이 바꿔준다. 따라서 카드를 뽑아서 score로 판단\n",
    "    Bool = self.use_ace()\n",
    "    if Bool == True: # ace를 11로 쓰겠다\n",
    "      self.z = 1\n",
    "      return int(sum(self.x)+10)\n",
    "    else :\n",
    "      self.z =0\n",
    "      return int((sum(self.x)))\n",
    "      \n",
    "  def draw_hand(self):  # 게임 시작 전, user, dealer set\n",
    "    deck = [1,2,3,4,5,6,7,8,9,10,10,10,10]\n",
    "    deck_user_one,deck_user_two = random.choice(deck),random.choice(deck) #user 2장 set\n",
    "    deck_dealer_one,deck_dealer_two = random.choice(deck),random.choice(deck) #dealer 2장 set\n",
    "    self.x.append(deck_user_one)\n",
    "    self.x.append(deck_user_two)  # user array에 저장\n",
    "    self.y = deck_dealer_one   # dealer가 보여줄 카드 선정\n",
    "    self.y_sum.append(deck_dealer_one)\n",
    "    self.y_sum.append(deck_dealer_two)  # dealer array에 저장\n",
    "    x_score = self.score() # ace-usable 판단 하면서, z set\n",
    "    y_score = self.score_y() # y의 score계산\n",
    "    while x_score<12 : # x는 12부터이므로, 12이상일때까지 hit 이때 ace유무를 판단해주자\n",
    "      self.x.append(random.choice(deck))\n",
    "      x_score = self.score() # ace-usable 판단 하면서, z set\n",
    "    while y_score<16 : #dealer는 sum이 16이하면 무조건 hit이므로 일단 다 뽑아 놓는다?\n",
    "      self.y_sum.append(random.choice(deck))\n",
    "      y_score = self.score_y()\n",
    "\n",
    "  def is_done(self):\n",
    "    if self.score()>21 :\n",
    "      return True  # is done\n",
    "    else :\n",
    "      return False\n",
    "  def user_hit_stand(self,action):  # 종료 조건을 고려하면서 action\n",
    "    deck = [1,2,3,4,5,6,7,8,9,10,10,10,10]\n",
    "    #action에 대한 보상을 주고 종료 조건 따지기\n",
    "    ys = self.score_y()\n",
    "    if ys > 21: # ace를 11로 쓰는데, 합이 21을 넘으면 dealer의 score는 0으로 가정\n",
    "      ys = 0\n",
    "    if action==1 : #hit\n",
    "      self.x.append(random.choice(deck)) \n",
    "      done = self.is_done() # 종료 조건 check\n",
    "      if done == True :  #user의 score가 21을 넘으면, score를 0으로 계산\n",
    "        if ys == 0 : #비겼을 때 \n",
    "          reward = 0\n",
    "        else :\n",
    "          reward = -1\n",
    "      else :\n",
    "        reward = -1\n",
    "      \n",
    "    elif action == 0 : #stand 무조건 종료\n",
    "      done = True\n",
    "      if ys == self.score() : #비겼을 때 \n",
    "        reward = 0\n",
    "      elif ys > self.score() : #lose\n",
    "        reward = -1\n",
    "      elif ys < self.score() : #win\n",
    "        reward = 1\n",
    "    #종료 조건\n",
    "    return ((self.score(),self.y,self.z),reward,done)  #observation , reward, done \n",
    "  \n",
    "  def current_state(self): # 현재 위치 제공\n",
    "    #print(self.x[-1],self.y,self.z)\n",
    "    return (self.score(),self.y,self.z)\n",
    "  def reset_state(self): # state initialize\n",
    "    self.x = []\n",
    "    self.y = 0\n",
    "    self.y_sum = []\n",
    "    self.z = 0\n",
    "agent_location = blackJack()\n",
    "agent_location.draw_hand()\n",
    "score = agent_location.score()\n",
    "print(\"x = {}\\nx_score = {}\\ny = {}\\nz = {}\".format(agent_location.x,score,agent_location.y,agent_location.z))\n",
    "print(\"y = {}\\ny_score = {}\".format(agent_location.y_sum,agent_location.score_y()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1187e1b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random # ====================================dealer가 user가 stop할 때 까지 wait\n",
    "import numpy as np\n",
    "\n",
    "class blackJack():\n",
    "  \n",
    "  def __init__(self): #trash value\n",
    "    self.x=[]  # user sum\n",
    "    self.y=0  # dealer show\n",
    "    self.y_sum =[]\n",
    "    self.z=0  # usable ace -> 11\n",
    "\n",
    "  def use_ace(self): # ace 유무 판단 -> 1이 있고,\n",
    "    if 1 in self.x and (sum(self.x) + 10 <= 21): \n",
    "      return True #ace를 11로 써도 된다.\n",
    "    elif 1 in self.x and (sum(self.x)+10 >21):\n",
    "      return False #ace를 가지고 있지만 1로 써야 한다.\n",
    "    else :\n",
    "      return False\n",
    "  \n",
    "  def dealer_ace(self) : # sum이 21을 넘지 않는 한 무조건 11로 계산\n",
    "    if 1 in self.y_sum and (sum(self.x) + 10 <= 21): \n",
    "      return True\n",
    "    elif 1 in self.y_sum and (sum(self.y_sum)+10 >21):\n",
    "      return False #ace를 가지고 있지만 1로 써야 한다.\n",
    "    else :\n",
    "      return False\n",
    "  def score_y(self): # ace 유무를 판단해서, 점수 합산 -> z도 같이 바꿔준다. 따라서 카드를 뽑아서 score로 판단\n",
    "    Bool2 = self.dealer_ace()\n",
    "    if Bool2 == True: # ace를 11로 쓰겠다\n",
    "      return int(sum(self.y_sum)+10)\n",
    "    else :\n",
    "      return int((sum(self.y_sum)))\n",
    "  def score(self): # ace 유무를 판단해서, 점수 합산 -> z도 같이 바꿔준다. 따라서 카드를 뽑아서 score로 판단\n",
    "    Bool = self.use_ace()\n",
    "    if Bool == True: # ace를 11로 쓰겠다\n",
    "      self.z = 1\n",
    "      return int(sum(self.x)+10)\n",
    "    else :\n",
    "      self.z =0\n",
    "      return int((sum(self.x)))\n",
    "      \n",
    "  def draw_hand(self):  # 게임 시작 전, user, dealer set\n",
    "    deck = [1,2,3,4,5,6,7,8,9,10,10,10,10]\n",
    "    deck_user_one,deck_user_two = random.choice(deck),random.choice(deck) #user 2장 set\n",
    "    deck_dealer_one,deck_dealer_two = random.choice(deck),random.choice(deck) #dealer 2장 set\n",
    "    self.x.append(deck_user_one)\n",
    "    self.x.append(deck_user_two)  # user array에 저장\n",
    "    self.y = deck_dealer_one   # dealer가 보여줄 카드 선정\n",
    "    self.y_sum.append(deck_dealer_one)\n",
    "    self.y_sum.append(deck_dealer_two)  # dealer array에 저장\n",
    "    x_score = self.score() # ace-usable 판단 하면서, z set\n",
    "    while x_score<12 : # x는 12부터이므로, 12이상일때까지 hit 이때 ace유무를 판단해주자\n",
    "      self.x.append(random.choice(deck))\n",
    "      x_score = self.score() # ace-usable 판단 하면서, z set\n",
    "   #user는 12이상일때 까지 다 뽑고, dealer는 2장만 뽑는다.\n",
    "  def is_done(self):\n",
    "    if self.score()>21 :\n",
    "      return True  # is done\n",
    "    else :\n",
    "      return False\n",
    "  def user_hit_stand(self,action):  # 종료 조건을 고려하면서 action\n",
    "    deck = [1,2,3,4,5,6,7,8,9,10,10,10,10]\n",
    "    #action에 대한 보상을 주고 종료 조건 따지기\n",
    "    if action==1 : #hit\n",
    "      self.x.append(random.choice(deck)) \n",
    "      done = self.is_done() # 종료 조건 check\n",
    "      reward = -1\n",
    "      \n",
    "    elif action == 0 : #stand 무조건 종료\n",
    "      done = self.is_done() #stop했는데 21을 넘는다?\n",
    "      if done == True:\n",
    "        reward = -1;\n",
    "      else : # stop했는데, 21을 넘지 않은 경우 -> dealer가 카드를 뽑고 우위를 따져야한다.\n",
    "      #dealer가 카드 뽑기\n",
    "        ys = self.score_y()\n",
    "        while ys<16 : #dealer는 sum이 16이하면 무조건 hit이므로 뽑는다.\n",
    "          self.y_sum.append(random.choice(deck))\n",
    "          ys = self.score_y()\n",
    "        if ys > 21: # ace를 11로 쓰는데, 합이 21을 넘으면 dealer의 score는 0으로 가정\n",
    "          ys = 0\n",
    "        if ys == self.score() : #비겼을 때 \n",
    "          reward = 0\n",
    "        elif ys > self.score() : #lose\n",
    "          reward = -1\n",
    "        elif ys < self.score() : #win\n",
    "          reward = 1\n",
    "        done = True #x가 21을 넘지 않아도, action이 stand이므로, game is done\n",
    "    #종료 조건\n",
    "    return ((self.score(),self.y,self.z),reward,done)  #observation , reward, done \n",
    "  \n",
    "  def current_state(self): # 현재 위치 제공\n",
    "    return (self.score(),self.y,self.z)\n",
    "  def reset_state(self): # state initialize\n",
    "    self.x = []\n",
    "    self.y = 0\n",
    "    self.y_sum = []\n",
    "    self.z = 0\n",
    "agent_location = blackJack()\n",
    "agent_location.draw_hand()\n",
    "score = agent_location.score()\n",
    "print(\"x = {}\\nx_score = {}\\ny = {}\\nz = {}\".format(agent_location.x,score,agent_location.y,agent_location.z))\n",
    "print(\"y = {}\\ny_score = {}\".format(agent_location.y_sum,agent_location.score_y()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fce380bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class QAgent_blackJack(): # y도 순서대로 뽑아야됨\n",
    "  def __init__(self):\n",
    "    self.q_value = np.zeros((10,10,2,2))  \n",
    "    self.eps = 0.9 # 0.9부터 -0.03을 통해 0.1까지 declaying greedy policy\n",
    "  \n",
    "  def select_action(self,state): # 어떤 action을 취할지 정하고, 현재 state에서 action\n",
    "    coin = random.random()\n",
    "    x,y,z = state\n",
    "    if coin > (1-self.eps+self.eps/2): \n",
    "      action = random.randint(0,1) #otherwise  0<= <=1 중 int 하나 가져오기 (hit, stand)\n",
    "    else : #현재 state에서 가장 q_value가 높은 방향으로 action\n",
    "      idx = self.q_value[x,y,z,:] \n",
    "      action = np.argmax(idx) # 값이 높은 index 가져오기\n",
    "    return action\n",
    "  \n",
    "  #def eps_declaying(self): #action을 하고 나서 앱실론 감소\n",
    "  #  self.eps -= 0.03\n",
    "  #  self.eps = max(self.eps,0.1) #앱실론은 최소 0.1까지 감소\n",
    "  def eps_declaying(self):\n",
    "    self.eps -= 0.03\n",
    "    self.eps = max(self.eps,0)\n",
    "  def update_q(self,transition):\n",
    "    alpha = 0.01 # learning rate\n",
    "    (x,y,z),reward,a,(x_prime,y_prime,z_prime) = transition\n",
    "    if x_prime > 9 or y_prime> 9  : # 범위를 벗어나면 추정치 q_value는 0으로 계산\n",
    "        self.q_value[x,y,z,a] = self.q_value[x,y,z,a] +alpha*(reward -self.q_value[x,y,z,a])\n",
    "    else :\n",
    "      a_prime = self.select_action((x_prime,y_prime,z_prime)) # 실제로 움직이는 건 아니고, 추정 action 뽑기\n",
    "      self.q_value[x,y,z,a] = self.q_value[x,y,z,a] +alpha*(reward+self.q_value[x_prime,y_prime,z_prime,a_prime] -self.q_value[x,y,z,a])\n",
    "\n",
    "def graph(user):\n",
    "  q = user.q_value\n",
    "  q1= []\n",
    "  q2= []\n",
    "  for i in range(0,10): # z =0 \n",
    "    for j in range(0,10):\n",
    "      q1.append(np.argmax(q[i][j][0]))\n",
    "      q2.append(np.argmax(q[i][j][1]))\n",
    "  q1 = np.array(q1)\n",
    "  q2 = np.array(q2)\n",
    "  q1 = q1.reshape(10,10)\n",
    "  q2 = q2.reshape(10,10)\n",
    "  x = np.array(range(1,11))\n",
    "\n",
    "  fig=plt.figure(figsize=(15,10))\n",
    "  plt.subplot(1, 2, 1)\n",
    "  plt.grid(True)\n",
    "  plt.axis([0,11,11,22])\n",
    "  plt.xticks(range(11))\n",
    "  plt.yticks(range(11,22))\n",
    "  plt.title(\"z = 0 \",fontsize = 25)\n",
    "  plt.xlabel(\"dealer show\",fontsize = 10)\n",
    "  plt.ylabel(\"user sum\",fontsize = 10)\n",
    "  for i in range(0,10):\n",
    "    y = []\n",
    "    for j in range(0,10):\n",
    "      if q1[i][j] ==1:\n",
    "        y.append(i+12)\n",
    "      else :\n",
    "        y.append(0)\n",
    "    plt.scatter(x,y,color='blue',marker = '*')\n",
    "  fig=plt.figure(figsize=(15,10))\n",
    "  plt.subplot(1, 2, 2)\n",
    "  plt.grid(True)\n",
    "  plt.axis([0,11,11,22])\n",
    "  plt.xticks(range(11))\n",
    "  plt.yticks(range(11,22))\n",
    "  plt.title(\"z = 1 \",fontsize = 25)\n",
    "  plt.xlabel(\"dealer show\",fontsize = 10)\n",
    "  plt.ylabel(\"user sum\",fontsize = 10)\n",
    "  for i in range(0,10):\n",
    "    y = []\n",
    "    for j in range(0,10):\n",
    "      if q2[i][j] ==1:\n",
    "        y.append(i+12)\n",
    "      else :\n",
    "        y.append(0)\n",
    "    plt.scatter(x,y,color='red',marker = '*')\n",
    "def main():\n",
    "  environment = blackJack()\n",
    "  user = QAgent_blackJack()\n",
    "  for episode in range(10000000):\n",
    "    done = False # 종료 조건 만족을 하는가?\n",
    "    environment.reset_state()# state 초기화\n",
    "    environment.draw_hand() # user & dealer get card\n",
    "    while not done: #policy evaluate\n",
    "      (x,y,z) = environment.current_state() # 현재 위치이자 action을 했을 때 q_value를 update할 state\n",
    "      x -= 12\n",
    "      y -= 1\n",
    "      action = user.select_action((x,y,z)) # 현재위치에서 action 정하기\n",
    "      (x_next,y_next,z_next),reward,done = environment.user_hit_stand(action) # 실제 action하기 -> state transition\n",
    "      x_next -= 12 \n",
    "      y_next -= 1\n",
    "       # agent가 정한 action을 envionment에게 전달하면, observation & reward  & done이 나온다.\n",
    "      user.update_q(((x,y,z),reward,action,(x_next,y_next,z_next))) #transition 전달\n",
    "    user.eps_declaying()\n",
    "  graph(user)\n",
    "\n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c82411a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "environment = blackJack()\n",
    "user = QAgent_blackJack()\n",
    "for episode in range(500000):\n",
    "  done = False # 종료 조건 만족을 하는가?\n",
    "  environment.reset_state()# state 초기화\n",
    "  environment.draw_hand() # user & dealer get card\n",
    "  while not done: #policy evaluate\n",
    "    (x,y,z) = environment.current_state() # 현재 위치이자 action을 했을 때 q_value를 update할 state\n",
    "    x -= 12\n",
    "    y -= 1\n",
    "    action = user.select_action((x,y,z)) # 현재위치에서 action 정하기\n",
    "    (x_next,y_next,z_next),reward,done = environment.user_hit_stand(action) # 실제 action하기 -> state transition\n",
    "    x_next -= 12 # 왜 이거 안되지\n",
    "    y_next -= 1\n",
    "       # agent가 정한 action을 envionment에게 전달하면, observation & reward  & done이 나온다.\n",
    "    user.update_q(((x,y,z),reward,action,(x_next,y_next,z_next))) #transition 전달\n",
    "  user.eps_declaying()\n",
    "\n",
    "q = user.q_value\n",
    "q1= []\n",
    "q2= []\n",
    "for i in range(0,10): # z =0 \n",
    "  for j in range(0,10):\n",
    "    a = np.argmax(q[i][j][0])\n",
    "    q1.append(q[i][j][0][a]) # value 가져 오기\n",
    "    b = np.argmax(q[i][j][1])\n",
    "    q2.append(q[i][j][1][b])\n",
    "q1 = np.array(q1)\n",
    "q2 = np.array(q2)\n",
    "q1 = q1.reshape(10,10)\n",
    "q2 = q2.reshape(10,10)\n",
    "x = np.array(range(0,10))\n",
    "y = np.array(range(12,22))\n",
    "fig = plt.figure(figsize=(9, 6))\n",
    "z = q1\n",
    "ax = fig.add_subplot(211, projection='3d')\n",
    "#ax.plot_surface(x, y, z, cmap=\"brg_r\")\n",
    "X, Y = np.meshgrid(x, y)\n",
    "surf = ax.plot_surface(X, Y, z, rstride=1, cstride=1, cmap=plt.cm.coolwarm, vmin=-1.0, vmax=1.0)\n",
    "ax.set_xlabel('Player\\'s Current Sum')\n",
    "ax.set_ylabel('Dealer\\'s Showing Card')\n",
    "ax.set_zlabel('State Value')\n",
    "ax.view_init(ax.elev, -120)\n",
    "\n",
    "fig = plt.figure(figsize=(10,10))\n",
    "z = q2\n",
    "ax = fig.add_subplot(212, projection='3d')\n",
    "X, Y = np.meshgrid(x, y)\n",
    "surf = ax.plot_surface(X, Y, z, rstride=1, cstride=1, cmap=plt.cm.coolwarm, vmin=-1.0, vmax=1.0)\n",
    "ax.set_xlabel('Player\\'s Current Sum')\n",
    "ax.set_ylabel('Dealer\\'s Showing Card')\n",
    "ax.set_zlabel('State Value')\n",
    "ax.view_init(ax.elev, -200)\n",
    "print(user.eps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f48cd1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "environment = blackJack()\n",
    "user = QAgent_blackJack()\n",
    "for episode in range(500000):\n",
    "  done = False # 종료 조건 만족을 하는가?\n",
    "  environment.reset_state()# state 초기화\n",
    "  environment.draw_hand() # user & dealer get card\n",
    "  while not done: #policy evaluate\n",
    "    (x,y,z) = environment.current_state() # 현재 위치이자 action을 했을 때 q_value를 update할 state\n",
    "    x -= 12\n",
    "    y -= 1\n",
    "    action = user.select_action((x,y,z)) # 현재위치에서 action 정하기\n",
    "    (x_next,y_next,z_next),reward,done = environment.user_hit_stand(action) # 실제 action하기 -> state transition\n",
    "    x_next -= 12 # 왜 이거 안되지\n",
    "    y_next -= 1\n",
    "       # agent가 정한 action을 envionment에게 전달하면, observation & reward  & done이 나온다.\n",
    "    user.update_q(((x,y,z),reward,action,(x_next,y_next,z_next))) #transition 전달\n",
    "  user.eps_declaying()\n",
    "\n",
    "q = user.q_value\n",
    "for i in range(0,10):\n",
    "  for j in range(0,10):\n",
    "    for k in range(0,10):\n",
    "      z = q[i][j][k][0]*0.1+q[i][j][k][1]*0.9\n",
    "q1= []\n",
    "q2= []\n",
    "for i in range(0,10): # z =0 \n",
    "  for j in range(0,10):\n",
    "    a = np.argmax(q[i][j][0])\n",
    "    q1.append(a) # value 가져 오기\n",
    "    b = np.argmax(q[i][j][1])\n",
    "    q2.append(b)\n",
    "q1 = np.array(q1)\n",
    "q2 = np.array(q2)\n",
    "q1 = q1.reshape(10,10)\n",
    "q2 = q2.reshape(10,10)\n",
    "x = np.array(range(12,22))\n",
    "y = np.array(range(0,10))\n",
    "fig = plt.figure(figsize=(9, 6))\n",
    "z = q1\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "ax.plot_surface(x, y, z, cmap=\"brg_r\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfcc5599",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QAgent_blackJack(): # y도 순서대로 뽑아야됨.\n",
    "  def __init__(self):\n",
    "    self.q_value = np.zeros((22,11,2,2))  \n",
    "    self.eps = 0.9 # 0.9부터 -0.03을 통해 0.1까지 declaying greedy policy\n",
    "  \n",
    "  def select_action(self,state): # 어떤 action을 취할지 정하고, 현재 state에서 action\n",
    "    coin = random.random()\n",
    "    x,y,z = state\n",
    "    #print(state)\n",
    "    if coin > (1-self.eps+self.eps/2): \n",
    "      action = random.randint(0,1) #otherwise  0<= <=1 중 int 하나 가져오기 (hit, stand)\n",
    "    else : #현재 state에서 가장 q_value가 높은 방향으로 action\n",
    "      idx = self.q_value[x,y,z,:] \n",
    "      action = np.argmax(idx) # 값이 높은 index 가져오기\n",
    "    return action\n",
    "  \n",
    "  def eps_declaying(self): #action을 하고 나서 앱실론 감소\n",
    "    self.eps -= 0.03\n",
    "    self.eps = max(self.eps,0.1) #앱실론은 최소 0.1까지 감소\n",
    "  \n",
    "  def update_q(self,transition):\n",
    "    alpha = 0.01 # learning rate\n",
    "    (x,y,z),reward,a,(x_prime,y_prime,z_prime) = transition\n",
    "    if x_prime > 21 or y_prime>10 : # 범위를 벗어나면 추정치 q_value는 0으로 계산\n",
    "        self.q_value[x,y,z,a] = self.q_value[x,y,z,a] +alpha*(reward -self.q_value[x,y,z,a])\n",
    "    else :\n",
    "      a_prime = self.select_action((x_prime,y_prime,z_prime)) # 실제로 움직이는 건 아니고, 추정 action 뽑기\n",
    "      self.q_value[x,y,z,a] = self.q_value[x,y,z,a] +alpha*(reward+self.q_value[x_prime,y_prime,z_prime,a_prime] -self.q_value[x,y,z,a])\n",
    "  def show_table(self):\n",
    "    q_lst = self.q_value.tolist()\n",
    "    data = np.zeros((22,11,2))\n",
    "    for row_idx in range(len(q_lst)):\n",
    "      row = q_lst[row_idx]\n",
    "      for col_idx in range(len(row)):\n",
    "        col = row[col_idx]\n",
    "        for z_idx in range(len(col)):\n",
    "          z = col[z_idx]\n",
    "          action = np.argmax(z)\n",
    "          data[row_idx,col_idx,z_idx] = action\n",
    "    print(data)\n",
    "def main():\n",
    "  environment = blackJack()\n",
    "  user = QAgent_blackJack()\n",
    "  for episode in range(1000):\n",
    "    done = False # 종료 조건 만족을 하는가?\n",
    "    environment.reset_state()# state 초기화\n",
    "    environment.draw_hand() # user & dealer get card\n",
    "    while not done: #policy evaluate\n",
    "      (x,y,z) = environment.current_state() # 현재 위치이자 action을 했을 때 q_value를 update할 state\n",
    "      action = user.select_action((x,y,z)) # 현재위치에서 action 정하기\n",
    "      (x_next,y_next,z_next),reward,done = environment.user_hit_stand(action) # 실제 action하기 -> state transition\n",
    "      #print(done)\n",
    "       # agent가 정한 action을 envionment에게 전달하면, observation & reward  & done이 나온다.\n",
    "      user.update_q(((x,y,z),reward,action,(x_next,y_next,z_next))) #transition 전달\n",
    "    user.eps_declaying()\n",
    "  user.show_table()\n",
    "\n",
    "main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
