{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82b5b6ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Intelligent Trajectory Design in UAV-Aided Communications With Reinforcement Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66a5fda9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import collections \n",
    "import torch.optim as optim\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive') #계정을 연동해야만 내가 만든 py에 접근할 수 있다.\n",
    "save_model_path = f\"/content/drive/MyDrive/Colab Notebooks/results/uav.pt\"\n",
    "save_model_path2 = f\"/content/drive/MyDrive/Colab Notebooks/results/uav_v_net.pt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b17a8b87",
   "metadata": {},
   "outputs": [],
   "source": [
    "#hyperparameters \n",
    "MINI_batch = 64\n",
    "buffer_size = 100000\n",
    "lr = 0.0005\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4c86a44",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random as rd\n",
    "class env_uav():\n",
    "  def __init__(self):\n",
    "    self.q = [[20.,20.,200.]]\n",
    "    self.V = 6\n",
    "    self.L = 3 \n",
    "    self.K = 5 \n",
    "    self.T = 60\n",
    "    self.N = 60\n",
    "    self.user = []\n",
    "    self.done = False\n",
    "    self.t = self.L \n",
    "    self.state = []\n",
    "  \n",
    "  def user_set(self):\n",
    "    #user random location\n",
    "    \n",
    "    for k in range(self.K):\n",
    "      self.user.append([rd.randint(150,180),rd.randint(150,180),0]) # 5 x 3(x,y,z)\n",
    "    return self.user\n",
    "\n",
    "  def cal_rate(self,t): #reward 구할 때\n",
    "    sum = 0   # time step t에서의 sum rate를 return 즉, 이게 수신 강도이자 reward\n",
    "    for i in range(self.K):#np.log2\n",
    "      sum += np.log2(1+1/np.sqrt(np.power(self.q[t][0]-self.user[i][0],2)+np.power(self.q[t][1]-self.user[i][1],2)+np.power(self.q[t][2]-self.user[i][2],2)))\n",
    "    return sum # 1+를 뺼까\n",
    "  \n",
    "  def state_rate(self,t): #state 구할 때\n",
    "    sum = 0   # time step t에서의 sum rate를 return 즉, 이게 수신 강도이자 reward\n",
    "    for i in range(self.K):#np.log2\n",
    "      sum += np.log2(1+1/np.sqrt(np.power(self.q[t][0]-self.user[i][0],2)+np.power(self.q[t][1]-self.user[i][1],2)+np.power(self.q[t][2]-self.user[i][2],2)))\n",
    "    return sum*1000 # 1+를 뺼까 log를 다시 씌울까\n",
    "\n",
    "\n",
    "  def q_location(self,a,t): #t+1의 observation  \n",
    "  #v, seta, low => dtype = tensor\n",
    "    v , seta , low = a\n",
    "    v = v.detach().numpy()*self.V; seta = seta.detach().numpy()*180; low = low.detach().numpy()*360\n",
    "\n",
    "    self.q.append([self.q[t][0] + np.sin(seta*np.pi/180)*np.cos(low*np.pi/180)*v\\\n",
    "                   ,self.q[t][1] + np.sin(low*np.pi/180)*np.sin(seta*np.pi/180)*v,\\\n",
    "                   self.q[t][2] + np.cos(seta*np.pi/180)*v])\n",
    "  \n",
    "  def init_state(self):\n",
    "    s_0 = self.state_rate(0) # q0에서의 e\n",
    "    state = []\n",
    "    for i in range(self.L):\n",
    "      init_action = torch.tensor([torch.rand(1),torch.rand(1),torch.rand(1)],dtype = torch.float)\n",
    "      self.q_location(init_action,i) # action받고 행동\n",
    "      s_1 = self.state_rate(i+1)\n",
    "      delta = (s_1 - s_0) # state가 너무 작아서 \n",
    "      state.append(delta)\n",
    "      s_0 = s_1\n",
    "\n",
    "    return state\n",
    "\n",
    "  def reset(self): # q, user,done initialize\n",
    "    self.user = []\n",
    "    self.q = [[20.,20.,200.]]\n",
    "    user = self.user_set()\n",
    "    self.done = False\n",
    "    return user\n",
    "\n",
    "  def current_uav(self): # 현재 uav 위치\n",
    "    return self.q\n",
    "\n",
    "  def step(self,a,s,t) : #action받으면 reward와 transition, done return \n",
    "    #a는 tensor\n",
    "    _,delta_2,delta_3 = s\n",
    "    state = []\n",
    "\n",
    "    s_0 = self.state_rate(t)  #t에서의 rate\n",
    "    self.q_location(a,t)  # t+1에서의 위치\n",
    "    s_1 = self.state_rate(t+1) # t+1에서의 rate\n",
    "    delta_1 = s_1 - s_0\n",
    "    delta_1 = delta_1 \n",
    "    delta_2 = delta_2 \n",
    "    delta_3 = delta_3 \n",
    "\n",
    "    state.append(delta_2)\n",
    "    state.append(delta_3)\n",
    "    state.append(delta_1)\n",
    " \n",
    "    r = self.cal_rate(t)\n",
    "\n",
    "    self.t += 1 #실제로 움직인다 -> time step이 증가 => T가 되면 종료\n",
    "    if self.t >= self.T:\n",
    "      self.done = True\n",
    "    return state, r, self.done # list(3,) , numpy, bool\n",
    " \n",
    "  def location(self,init_s,policy,q_lo):\n",
    "      q_rate = 0\n",
    "      s = init_s\n",
    "      self.q = q_lo\n",
    "        \n",
    "      for i in range(3,60):\n",
    "        a =policy(torch.tensor(s).float())   \n",
    "        s_prime,r,_ = self.step(a,s,t) # noise가 없는 action으로 움직였을 때의 reward를 따로 계산\n",
    "        q_rate += r\n",
    "      \n",
    "      return q_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb1a1001",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VN(nn.Module):\n",
    "  def __init__(self):\n",
    "    super().__init__()\n",
    "    self.fc_1 = nn.Linear(6,256)\n",
    "    self.fc_2 = nn.Linear(256,256)\n",
    "    self.fc_3 = nn.Linear(256,256)\n",
    "    self.fc_4 = nn.Linear(256,256)\n",
    "    self.fc_5 = nn.Linear(256,256)\n",
    "    self.fc_6 = nn.Linear(256,1)\n",
    "\n",
    "    self.optimizer = optim.Adam(self.parameters(),lr = lr)\n",
    "  def forward(self,x):\n",
    "    x = F.relu(self.fc_1(x))\n",
    "    x = F.relu(self.fc_2(x))\n",
    "    x = F.relu(self.fc_3(x))\n",
    "    x = F.relu(self.fc_4(x))\n",
    "    x = F.relu(self.fc_5(x))\n",
    "    x = self.fc_6(x) \n",
    "    return x\n",
    "\n",
    "class PN(nn.Module):\n",
    "  def __init__(self):\n",
    "    super().__init__()\n",
    "    self.fc1 = nn.Linear(3,256)\n",
    "    self.fc2 = nn.Linear(256,256)\n",
    "    self.fc3 = nn.Linear(256,256)\n",
    "    self.fc4 = nn.Linear(256,256)\n",
    "    self.fc5 = nn.Linear(256,256)\n",
    "    self.fc6 = nn.Linear(256,3)\n",
    "\n",
    "    self.buffer = collections.deque(maxlen = buffer_size)\n",
    "    self.optimizer = optim.Adam(self.parameters(),lr = lr)\n",
    "\n",
    "  def forward(self,x):\n",
    "    x = F.relu(self.fc1(x))\n",
    "    x = F.relu(self.fc2(x))\n",
    "    x = F.relu(self.fc3(x))\n",
    "    #x = F.relu(self.fc4(x))\n",
    "    #x = F.relu(self.fc5(x))\n",
    "    x = F.sigmoid(self.fc6(x)) \n",
    "    return x\n",
    "\n",
    "  def put_data(self,transition):   \n",
    "    self.buffer.append(transition)\n",
    "\n",
    "  def sampling(self,batch_size):\n",
    "    mini_batch = rd.sample(self.buffer,batch_size) #buffer에서 batch_size만큼 random하게 sampling\n",
    "    #batch_size x 4\n",
    "    #list, array, list, numpy, bool\n",
    "    s_list , a_list , r_list , s_prime_list , done_list = [], [] ,[] ,[] ,[] \n",
    "\n",
    "    for transition in mini_batch:\n",
    "      s, a, r, s_prime, done = transition\n",
    "    \n",
    "      s_list.append(s)\n",
    "      a_list.append(a)\n",
    "      r_list.append([r])\n",
    "      s_prime_list.append(s_prime)\n",
    "      done_list.append([done])\n",
    "    \n",
    "    return torch.tensor(s_list,dtype=torch.float), torch.tensor(a_list,dtype = torch.float),\\\n",
    "    torch.tensor(r_list,dtype = torch.float),torch.tensor(s_prime_list,dtype = torch.float),\\\n",
    "     torch.tensor(done_list,dtype = torch.float)\n",
    "\n",
    "  def size(self):\n",
    "    return len(self.buffer)\n",
    "\n",
    "  def train(self,V_net,V_target,P_target):\n",
    "    for i in range(10):\n",
    "      s, a, r, s_prime, done = self.sampling(MINI_batch)\n",
    "\n",
    "      \n",
    "\n",
    "      a_prime = P_target.forward(s_prime) # a'\n",
    "      s_a = torch.hstack((s,a)) # Q_value network의 input\n",
    "      s_prime_a = torch.hstack((s_prime,a_prime))\n",
    "\n",
    "      value_s_prime = V_target(s_prime_a) # tensor\n",
    "      td_target = r + value_s_prime*done #\n",
    "      value_s = V_net(s_a) # batch x 1  tensor\n",
    "\n",
    "      V_loss = F.smooth_l1_loss(value_s,td_target.detach())\n",
    "      V_net.optimizer.zero_grad()\n",
    "      V_loss.backward()\n",
    "      V_net.optimizer.step()\n",
    "      \n",
    "      a_P = self.forward(s)\n",
    "      s_a2 = torch.hstack((s,a_P))\n",
    "      P_loss = -torch.mean(V_net(s_a2))\n",
    "      self.optimizer.zero_grad()\n",
    "      P_loss.backward()\n",
    "      self.optimizer.step()\n",
    "  \n",
    "    return V_loss.item(), P_loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8073382a",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = env_uav() \n",
    "policy = PN()\n",
    "policy.load_state_dict(torch.load(save_model_path)) # training된 parameter load \n",
    "policy_target = PN() \n",
    "\n",
    "value = VN()\n",
    "value.load_state_dict(torch.load(save_model_path2))\n",
    "value_target = VN()\n",
    "\n",
    "policy_target.load_state_dict(policy.state_dict())\n",
    "value_target.load_state_dict(value.state_dict())\n",
    "\n",
    "\n",
    "M = 50000\n",
    "avgv_loss =0\n",
    "avgp_loss =0\n",
    "sum_rate =0 \n",
    "avg_t = 100\n",
    "V_loss = 0\n",
    "P_loss = 0\n",
    "temp = []\n",
    "action_list = []\n",
    "\n",
    "for m in range(M):\n",
    "  #user generate, p, done 초기화\n",
    "  user = env.reset()\n",
    "  #action list 초기화\n",
    "  action_list = []\n",
    "  #1 ~ L 까지 random action =======================================\n",
    "  s = env.init_state() #(3,)\n",
    "  init_s = s\n",
    "  q_lo = env.current_uav()\n",
    "  done = False\n",
    "\n",
    "  for t in range(3,60):\n",
    "    action =policy(torch.tensor(s).float()) \n",
    "    action_list.append((action.detach().numpy())) # noise가 없는 깨끗한 action 저장\n",
    "    v, seta, low = torch.randn(1)/6, torch.rand(1)/180,torch.rand(1)/360\n",
    "    #print(s)\n",
    "    #print(action)\n",
    "    a = action\n",
    "    a.data[0] += v[0]; a.data[1] += seta[0]; a.data[2] += low[0];\n",
    "    s_prime , reward, _ = env.step(a,s,t)  \n",
    "    \n",
    "    # list, numpy, bool\n",
    "    done_num = 1.0\n",
    "    if t == 60 : # 사실 for문이 다 돌면 끝난다.\n",
    "      done_num =0.0\n",
    "      done = True\n",
    "\n",
    "    a = action.detach().numpy()\n",
    "    policy.put_data((s,a,reward, s_prime,done_num)) # buffer에 저장\n",
    "    s = s_prime\n",
    "\n",
    "    if done == True:\n",
    "      break\n",
    "  \n",
    "  if policy.size() > 10000:\n",
    "    V_loss,P_loss = policy.train(value,value_target,policy_target)\n",
    "  \n",
    "  avgv_loss += V_loss\n",
    "  avgp_loss += P_loss\n",
    "  epi_rate = 0 # 종료 조건;\n",
    "  temp_rate = 0\n",
    "  #noise가 없는 action으로, 위치를 계산해야 된다.\n",
    "  epi_rate = env.location(init_s,policy,q_lo)\n",
    "\n",
    "  temp.append(epi_rate)   #avg_t마다 갱신\n",
    "  sum_rate = np.sum(temp)\n",
    "  temp_rate = np.max(temp)\n",
    "\n",
    "  if temp_rate > 100.0 : #제대로 된 policy network로 reward계산을 해야된다.\n",
    "    torch.save(policy.state_dict(),save_model_path)\n",
    "    torch.save(value.state_dict(),save_model_path2)\n",
    "    print(\"epi_rate\",temp_rate)\n",
    "    break\n",
    "\n",
    "  if m%avg_t == 0 and m!=0 :\n",
    "    #print(reward)\n",
    "    policy_target.load_state_dict(policy.state_dict())\n",
    "    value_target.load_state_dict(value.state_dict())\n",
    "    print(\"episode : {}, buffer_size : {}, v_loss : {}, p_loss : {}, avg_sum_rate : {} , epi_rate = {}\"\\\n",
    "          .format(m,policy.size(),avgv_loss/avg_t,avgp_loss/avg_t,sum_rate/avg_t,temp_rate))\n",
    "    avgv_loss = 0\n",
    "    avgp_loss = 0\n",
    "    sum_rate = 0 \n",
    "    temp = []                              # deep한 정도                                                   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb89de8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "location = env.current_uav()\n",
    "location = np.array(location)\n",
    "print(location)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "355c0f3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "fig =  plt.figure(figsize=[10,9])\n",
    "ax=fig.add_subplot(1,1,1, projection='3d')\n",
    "\n",
    "loc = np.array(location)\n",
    "user = np.array(user)\n",
    "print(loc.shape)\n",
    "print(user.shape)\n",
    "ax.scatter(loc[:,0],loc[:,1],loc[:,2],'ro')\n",
    "ax.scatter(user[:,0],user[:,1],user[:,2],'b*')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eb5e95c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b4cc0fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = PN() # model class 정의해주고,\n",
    "model.load_state_dict(torch.load(save_model_path)) #parameter load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39dd04a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#user generate, p, done 초기화\n",
    "env = env_uav()\n",
    "user = env.reset()\n",
    "#1 ~ L 까지 random action =======================================\n",
    "s = env.init_state()\n",
    "sum_rate =0 \n",
    "\n",
    "for t in range(3,60):\n",
    "  action =model(torch.tensor(s).float()) \n",
    "  s_prime , reward, _ = env.step(action,s,t)  \n",
    "  s = s_prime\n",
    "  \n",
    "for i in range(60):\n",
    "    sum_rate += env.cal_rate(i)\n",
    "\n",
    "location = env.current_uav()\n",
    "location = np.array(location)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b92b108",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sum_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53f3386b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(user)\n",
    "print(location)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d0760f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "fig =  plt.figure(figsize=[10,9])\n",
    "ax=fig.add_subplot(1,1,1, projection='3d')\n",
    "\n",
    "loc = np.array(location)\n",
    "user = np.array(user)\n",
    "\n",
    "ax.scatter(loc[:,0],loc[:,1],loc[:,2],'ro')\n",
    "ax.scatter(user[:,0],user[:,1],user[:,2],'b*')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
